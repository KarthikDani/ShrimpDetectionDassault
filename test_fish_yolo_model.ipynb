{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**\n",
    "\n",
    "In this cell, we import the necessary libraries for data handling, image processing, visualization, and file management.\n",
    "#### **Explanation**:\n",
    "- `PIL` and `PIL.Image`: For image processing tasks.\n",
    "- `IPython.display.display`: For displaying images within the notebook.\n",
    "- `glob`: For file pattern matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL \n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import random\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing a Random Sample of Images**\n",
    "\n",
    "This cell randomly sample a specified number of images from the validation dataset and display them in a grid for visual inspection.\n",
    "\n",
    "#### **Explanation**:\n",
    "- `root_path`: Path pattern to load images from the validation directory.\n",
    "- `num_samples`: Number of random images to display.\n",
    "- `images_data`: List of all image file paths sorted alphabetically.\n",
    "- `random_images`: Randomly sampled image paths.\n",
    "- `num_rows`: Calculated number of rows needed to display the images.\n",
    "- `plt.figure()`: Initializes the figure with a specific size.\n",
    "- `plt.subplot()`: Defines the subplot layout for each image.\n",
    "- `plt.imshow()`: Displays the image.\n",
    "- `plt.axis('off')`: Hides the axis for a cleaner display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'cleaned_data/validation/images/*'\n",
    "num_samples = 200\n",
    "images_data = sorted(glob(root_path))\n",
    "random_images = random.sample(images_data, num_samples)\n",
    "\n",
    "num_rows = num_samples // 2\n",
    "plt.figure(figsize=(16, 8 * num_rows)) \n",
    "\n",
    "print(\"Num rows:\", num_rows)\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(num_rows, 2, i + 1)\n",
    "    plt.imshow(cv2.imread(random_images[i]))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading a YOLOv8 Model**\n",
    "\n",
    "In this cell, we load a pre-trained YOLOv8 model from a specified weights file.\n",
    "\n",
    "#### **Explanation**:\n",
    "- `from ultralytics import YOLO`: Imports the YOLO class from the Ultralytics library.\n",
    "- `model = YOLO('best.pt')`: Initializes a YOLOv8 model with pre-trained weights specified in `'best.pt'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO('best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Predicting Objects with YOLOv8 and Displaying Results**\n",
    "\n",
    "In this cell, we use the YOLOv8 model to predict objects in randomly sampled images, process the detection outputs, and print details about detected objects.\n",
    "\n",
    "#### **Explanation**:\n",
    "- `yolo_outputs = model.predict(random_images[i])`: Runs the YOLOv8 model on the image to get predictions.\n",
    "- `output = yolo_outputs[0]`: Extracts the first output (in case there are multiple outputs).\n",
    "- `box = output.boxes`: Retrieves bounding box information.\n",
    "- `names = output.names`: Gets the class names for detected objects.\n",
    "- `for key, value in names.items()`: Updates class names to \"FISH\".\n",
    "- `for j in range(len(box))`: Iterates over each detected object.\n",
    "  - `labels = names[box.cls[j].item()]`: Gets the label for the detected class.\n",
    "  - `coordinates = box.xyxy[j].tolist()`: Gets the bounding box coordinates.\n",
    "  - `confidence = np.round(box.conf[j].item(), 2)`: Gets the confidence score.\n",
    "- `images.append(output.plot()[:, :, ::-1])`: Stores the image with detected bounding boxes in the `images` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    yolo_outputs = model.predict(random_images[i])\n",
    "    output = yolo_outputs[0]\n",
    "    box = output.boxes\n",
    "    names = output.names\n",
    "\n",
    "    for key, value in names.items():\n",
    "        names[key] = \"FISH\"\n",
    "    \n",
    "    for j in range(len(box)):\n",
    "        labels = names[box.cls[j].item()]\n",
    "        coordinates = box.xyxy[j].tolist()\n",
    "        confidence = np.round(box.conf[j].item(), 2)\n",
    "        \n",
    "        print(f'In this image {len(box)} {labels} has been detected.')\n",
    "        print(f'Coordinates are: {coordinates}')\n",
    "        print(f'Confidence is: {confidence}')\n",
    "        print('-------')\n",
    "        \n",
    "    # Store the image in the 'images' list\n",
    "    images.append(output.plot()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Annotating Images with YOLOv8 Predictions and Displaying Results**\n",
    "\n",
    "In this cell, we use the YOLOv8 model to predict objects in images, draw bounding boxes and labels on the images, and then display these annotated images.\n",
    "\n",
    "#### **Explanation**:\n",
    "- `yolo_outputs = model.predict(cv2.imread(random_images[i]))`: Loads and processes the image using YOLOv8.\n",
    "- `img = cv2.imread(random_images[i])`: Reloads the image to draw bounding boxes and labels.\n",
    "- `for j in range(len(box))`: Iterates through each detected object.\n",
    "  - `cv2.rectangle()`: Draws the bounding box on the image.\n",
    "  - `cv2.putText()`: Adds a label and confidence score to the image.\n",
    "- `images.append(img)`: Adds the annotated image to the `images` list.\n",
    "- `plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))`: Displays the annotated image using Matplotlib, converting from BGR to RGB color space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    yolo_outputs = model.predict(cv2.imread(random_images[i]))  # Load image using cv2.imread()\n",
    "    output = yolo_outputs[0]\n",
    "    box = output.boxes\n",
    "    names = output.names\n",
    "\n",
    "    for key, value in names.items():\n",
    "        names[key] = \"FISH\"\n",
    "    \n",
    "    img = cv2.imread(random_images[i])  # Load the image again to draw on it\n",
    "    \n",
    "    for j in range(len(box)):\n",
    "        labels = names[box.cls[j].item()]\n",
    "        coordinates = box.xyxy[j].tolist()\n",
    "        confidence = np.round(box.conf[j].item(), 2)\n",
    "        \n",
    "        # Draw bounding box and label on the image\n",
    "        pt1 = (int(coordinates[0]), int(coordinates[1]))\n",
    "        pt2 = (int(coordinates[2]), int(coordinates[3]))\n",
    "        cv2.rectangle(img, pt1, pt2, (0, 255, 0), 2)\n",
    "        cv2.putText(img, f'{labels} {confidence}', (pt1[0], pt1[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "    # Store the annotated image in the 'images' list\n",
    "    images.append(img)\n",
    "\n",
    "# Now 'images' list contains images with bounding boxes and labels drawn on them\n",
    "# Display each image in the images list\n",
    "for img in images:\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for matplotlib\n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a Video from Annotated Images**\n",
    "\n",
    "In this cell, we generate a video file by combining a sequence of annotated images.\n",
    "\n",
    "#### **Explanation**:\n",
    "- `output_video = cv2.VideoWriter('output_video.avi', cv2.VideoWriter_fourcc(*'MJPG'), 1, (img.shape[1], img.shape[0]))`: Initializes the video writer with:\n",
    "  - `'output_video.avi'`: Output video file name.\n",
    "  - `cv2.VideoWriter_fourcc(*'MJPG')`: Codec for video encoding (MJPEG).\n",
    "  - `1`: Frame rate (1 frame per second).\n",
    "  - `(img.shape[1], img.shape[0])`: Frame size (width x height of the images).\n",
    "- `output_video.write(img)`: Writes each image to the video file.\n",
    "- `output_video.release()`: Finalizes and closes the video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Define the video writer\n",
    "output_video = cv2.VideoWriter('output_video.avi', cv2.VideoWriter_fourcc(*'MJPG'), 1, (img.shape[1], img.shape[0]))\n",
    "\n",
    "# Write each image in the images list to the video\n",
    "for img in images:\n",
    "    output_video.write(img)\n",
    "\n",
    "# Release the video writer\n",
    "output_video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
